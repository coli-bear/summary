오픈소스 클러스터 컴퓨팅 프레임워크로 암시적 데이터 병렬성(Data Parallelism)과 장애허용(Fault Tolerance)과 더불어 완전한 클러스터를 프로그래밍하기 위한 인터페이스를 제공한다. 

# 1. 등장 배경

MapReduce 형태의 클러스터링 컴퓨팅 패러다임의 단점을 보완하기위해 등장 하였다. 

### 1.1. MapReduce

Disk 로부터 데이터를 읽은후 키-값(Key-Value) 형태로 연관성되어 있는 데이터 (Map)를 분류한 다음 Reduce 를 하여 중복데이터를 제거하고 원하는 데이터를 가공하여 다시 Disk 에 저장한다. 

MapReduce 는 파일을 읽고 쓰는 방식(Disk I/O) 을 체택하고 있어 성능이 좋지 않다. 

- 기본적으로 보조메모리(SSD, HDD) IO(Input Output) 은 주메모리 (RAM) 으로부터 데이터를 가져오는것보다 성능이 좋지 못하다.

이러한 단점을 보완하고 In Memory 기반의 연선을 통해 처리 성능을 향상시키고자 등장한것이 Spark 이다. 


## 2. Apache Spark  ?

아파치 스파크에서는 RDD, Dataframe, Dataset 의 3가지 API 를 제공하며 이러한 데이터를 바탕으로 In-Memory 연산 가능하도록 제공하여 보조메모리 기반의 Hadoop 에 비해 성능을 약 100배정도 끌어 올렸다.

아파치 스파크는 두개의 기본 데몬과 클러스터 관리자가 있는 Master-Slave 구조를 따른다.

- Master Daemon : Master and Driver Process
- Worker Daemon : Slave Process
- Cluster Manager

![[spark_architecture.png]]

스파크 클러스터에는 단일 Master 와 원한은 수의 Worker 가 있으며 Driver 와 Executor 는 개발 Java 프로세스를 실행하며 사용자는 이를 동일한 수평 Spark 클러스터 또는 별도의 시스템 또는 혼합 시스템 구성에서 사용할 수 있다. (사실 이 부분은 이해가 잘 안된다. 써보면 알지 않을까 싶다)```bash
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
```

아파치 스파크는 기본적으로 필요한 것이 두 가지가 있다.

## 2.1. Cluster Manager

클러스터를 관리하기 위해 사용되는것으로 대표적으로 아래 두가지 솔루션이 있다. 

- Apache Hadoop YARN 
- Apache Mesos

## 2.2. Distribute Storage System

데이터를 분산 저장하는 솔루션들로는 다음과 같은것들이 있다. 

- HDFS(Hadoop Distributed File System)
- MapR-FS(MapR File System)
- Cassandra
- OpenStack Swift
- Amazone S3
- Kudu
- Custom Solution (예 : WekaIO)

가장 많이 사용되는 솔루션으로는 Hadoop이며, zlib 와 Bzip2 같은 압축 알고리즘을 지원하며, Spark 와 같은 머신에서 구동가능하기 때문이다. 

## 2.3. Apache Spark API 

아파치 스파크에서 제공하는 RDD, Dataframe Dataset 이 세가지에 대해 간략하게 정리해보자

### 2.3.1. RDD

초기 정식 출시된 스파크 v1 이 제공한 API 로 인메모리 데이터 처리를 통하여 속도를 높일 수 있었지만 테이블 조인 효율화 같은 처리를 사용자가 직접해야해 최적화에 어려움이 있음

```javascript
val data = Array(1, 2, 3, 4, 5) val distData = sc.parallelize(data) distData.map(x => if(x >= 3) x else 0).reduce((x, y) => x + y)
```

### 2.3.2. DataFrame

스파크 1.3 에서 등장한 데이터 프레임은 처리속도증가와 프로젝트 텅스텐의 일부로 소개되었다. 데이터를 스키마형태로 추상화하고, 카탈리스트 옵티마이저가 쿼리를 최적화하여 처리함. 

```javascript 
val df = spark.read.json("examples/src/main/resources/people.json") df.select($"name", $"age").filter($"age" > 20).show() df.groupBy("age").count().show()
```

> [프로젝트 텅스텐](https://thebook.io/006908/0194/) : 스파크 1.5와 함께 공개했다. 스파크의 메모리 관리 방식을 완전히 개역하여 정렬, 집계, 셔플링 성능이 대폭 개선되었으며 스파크 1.5 부터는 텅스텐을 기본으로 활성화 한다 .

### 2.3.3. Dataset

스파크 1.6에서 추가되었으며 데이터의 타입체크, 직렬화를 위한 인코더, 카탈리스트 옵티마이저를 제공하므로써 데이터 처리 속도를 더욱 증가 시켰다. 

스파크 2.0 에서는 `Dataframe` 과 `Dataset`이 통합되었으며, 스칼라 API 에서 `Dataset[Row]` 는 `Dataframe`을 의미합니다.

```javascript
val path = "examples/src/main/resources/people.json" case class Person(name:String, age:Long) val peopleDS = spark.read.json(path).as[Person] peopleDS.show()
```

![[spark_2.0_api.png]]


# 3. 다른 시스템과의 결합

## 3.1. Spark with Apache Hadoop

아파치 스파크는 하둡을 대체하기 위해 나온것이 아니라 함께 사용하여 성능을 높이고자 나온 것이다. 그렇기 떄문에 하둡의 사용자들이 쉽게 스파크를 사용하도록 초점을 두어 개발하였다. 

그렇기 때문에 하둡의 버전/클러스터에대한 관리자 권한과는 무관하게 스파크를 실행할 수 있다. 

물론 Standalone 형태로 제공하지만 실제 운영에서는 주로 하둡과 결합되고 아래 3가지 형태로 하둡과 같이 사용할 수 있다. 

			![[spark_with_hardoop.png]]


### 3.1.1. Standalone Deployment 

하둡 클러스터의 모든 시스템 또는 하위시스템에 Resource 를 동적으로 할당하고 하둡 맵 리듀스와 함께 스파크를 실행할 수 있으며 사용자는 HDFS 데이터에서 임의의 스파크 작업을 실행할 수 있다. 

### 3.1.2. Hadoop Yarn Deployment 

하둡 YARN 을 배포했거나 배포하려는 사용자는 사전 설치나 관리 액세스 없이 스파크를 실행할수 있으며 이를통해 사용자는 스파크를 하둡에 쉽게 통합시켜 기능을 사용할수 있다. 

### 3.1.3. Spark In MapReduce(SIMR)

YARN 을 아직 시작하지 않은 하둡 사용자는 Standalone 외에 SIMR 을 통해 맵리듀스 안에서 스파크를 사용할 수 있다.

SIMR 을 사용하면 사용자는 스파크를 실험하고 다운로드한 후 몇분 내에 쉘을 사용할 수 있으며 진입장벽이 낮아 모두가 스파크를 사용할 수 있게 한다. 

## 3.2. Spark with Apache Hadoop YARN 

실제로 스파크의 작업을 구동하는 `Spark Executors` 와 `Executors` 를 스케줄링을 위한 `Spark Driver` 로 구성된 `Spark Jobs` 는 두가지 배치 모드중 한가지로 실행될 수 있다. 

이것을 이해해야 적당한 메모리를 할당하고 `Jobs`를 예상한대로 제출할수 있다. 


### 3.2.1. Cluster Mode

클러스터 모드는 모든것을 클러스터에서 구동한다. Job 을 클라이언트에서 구동할 수 있으며 클라이언트가 종료되어도 클러스터에서 Job 은 처리된다. 

`Spark Driver` 는 YARN Application Master 내부에 캡슐화 되어있으며 오랜 작업시간이 필요한 경우 해당 모드를 사용하는것이 좋다. 

	![[spark_cluster_mode.png]]

### 3.2.2. Client Mode

`Spark Driver` 이 클라이언트에서 구동되며 만약 클라이언트가 종료되면 Job 은 실패하게 된다. 반면 `Spark Executors` 는 여전히 `Cluster` 에서 되어 스케줄링을 위해 YARN Application Master 이 생성된다. 

클라이언트 모드는 Interactive Jobs (실시간 쿼리 or 온라인 데이터 분석 등 )에 적합하다. 

	![[spark_client_mode.png]]

### 3.3.3. 비교 

| 구분              | Cluster Mode                    | Client Mode |
| --------------- | ------------------------------- | ----------- |
| Spark Driver 위치 | Yarn Application Master 내부에 캡슐화 | 각각의 클라이언트   |
| 작업실행 주체         | 클라이언트                           | 클라이언트       |
| 클라이언트 종료시 작업 진행 | 클러스터에서 작업이 처리                   | 작업은 실패      |

# 4. 설치

회사내부망에 구축해보고 사용해볼 예정이다. 
## 4.1. 전제조건

먼저 회사 내부망에 Ubuntu Server 을 구축 후 실행할 예정이므로 VM 이 구축되어있는 서버가 있어야 할 것이다. 만약 내부에 개발용 서버가 없다면 AWS EC2 m4.xlarge 인스턴스를 생성하고 SSH와 HTTP 포트를 허용해준다. 

- **OS** : Linux/Ubuntu 가 개발 및 배포 플랫폼으로 지원
- **Storage** : 20 GB 의 최소 여유공간
- **Memory** : 최소 8 GB  RAM 




# 참고

- [\[Spark\] Apache Spark란?](https://mangkyu.tistory.com/128)
- [텅스텐 프로젝트의 스파크 성능 향상](https://thebook.io/006908/0194/)
- [RDD/Dataframe/Dataset](https://wikidocs.net/28377)
- [Spark RDDs vs DataFrames vs SparkSQL](https://dadk.tistory.com/74)
- [[Spark] Spark deploy mode 설정(Cluster/Client)](https://pickwon.tistory.com/155)

